img_size: 32
data_path: "./data"

CLIP:
  batch_size: 256
  epochs: 50
  lr: 3.0e-4
  embed_dim: 256

  # Image encoder
  vision_layers: [1, 2, 2, 2]
  vision_width: 16
  vision_patch_size: null

  # Text encoder
  context_length: 33
  vocab_size: 49408
  transformer_width: 64
  transformer_heads: 16
  transformer_layers: 8

  model_path: "./models/clip3.pth"

Decoder:
  # Define hyperparameters
  diffusion_timesteps: 300
  batch_size: 64
  epochs: 1500
  lr: 0.001
  # grad_clip: 0.005
  grad_clip: 3
  null_text_emb_rate: 0.2
  null_clip_emb_rate: 0.2
  guidance_scale: 3

  # UNet
  down_channels: [64, 128, 256]
  time_emb_dim: 32

  # UNet Transformer
  n_vocab: 49408
  context_length: 33
  transformer_width: 64
  transformer_layers: 3
  transformer_heads: 8

  # UNet attention block
  qkv_heads: 8

  normalize_clip_embeddings: True
  model_path: "./models/decoder3.pth"

Prior:
  # Define hyperparameters
  diffusion_timesteps: 200
  batch_size: 256
  epochs: 100
  lr: 0.01
  context_length: 33
  normalize_clip_embeddings: True

  depth: 8
  dim_per_head: 64
  heads: 16
  ff_mult: 4

  model_path: "./models/prior3.pth"
