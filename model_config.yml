img_size: 32

CLIP:
  batch_size: 128
  epochs: 300
  lr: 5.0e-4
  embed_dim: 48

  # Image encoder
  vision_layers: [1, 1, 1, 1]
  vision_width: 32
  vision_patch_size: null

  # Text encoder
  context_length: 33
  vocab_size: 49408
  transformer_width: 64
  transformer_heads: 8
  transformer_layers: 3

  model_path: "./models/clip.pth"

Decoder:
  # Define hyperparameters
  diffusion_timesteps: 300
  batch_size: 128
  epochs: 1000
  lr: 0.001
  grad_clip: 0.005
  null_text_emb_rate: 0.5
  null_clip_emb_rate: 0.1
  guidance_scale: 2

  # UNet
  down_channels: [64, 128, 256]
  time_emb_dim: 32

  # UNet Transformer
  n_vocab: 49408
  context_length: 33
  transformer_width: 32
  transformer_layers: 3
  transformer_heads: 4

  # UNet attention block
  qkv_heads: 4
