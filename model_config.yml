img_size: 32

CLIP:
  batch_size: 128
  epochs: 300
  lr: 5.0e-4
  embed_dim: 6

  # Image encoder
  vision_layers: [1, 1, 1, 1]
  vision_width: 32
  vision_patch_size: null

  # Text encoder
  context_length: 33
  vocab_size: 49408
  transformer_width: 64
  transformer_heads: 8
  transformer_layers: 3

  model_path: "./models/clip1.pth"

Decoder:
  # Define hyperparameters
  diffusion_timesteps: 300
  batch_size: 64
  epochs: 1500
  lr: 0.001
  # grad_clip: 0.005
  grad_clip: 1
  null_text_emb_rate: 0.2
  null_clip_emb_rate: 0.2
  guidance_scale: 3

  # UNet
  down_channels: [64, 128, 256]
  time_emb_dim: 32

  # UNet Transformer
  n_vocab: 49408
  context_length: 33
  transformer_width: 64
  transformer_layers: 3
  transformer_heads: 8

  # UNet attention block
  qkv_heads: 8

  normalize_clip_embeddings: True
  model_path: "./models/decoder1.pth"

Prior:
  # Define hyperparameters
  diffusion_timesteps: 200
  batch_size: 256
  epochs: 100
  lr: 0.01
  context_length: 33
  normalize_clip_embeddings: True

  depth: 8
  dim_per_head: 64
  heads: 16
  ff_mult: 4

  model_path: "./models/prior1.pth"
